{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb5a558",
   "metadata": {},
   "source": [
    "#### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92877305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "\n",
    "#Tests for sample comparison\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import shapiro\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "\n",
    "#ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "\n",
    "#Prophet\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "from prophet.diagnostics import cross_validation\n",
    "\n",
    "#LightGBM\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "\n",
    "#XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "#Libraries for tuning NHITS and envieroment setup\n",
    "import logging\n",
    "from ray import tune\n",
    "from IPython.display import display\n",
    "\n",
    "#NHITS\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import LSTM, NHITS\n",
    "from neuralforecast.auto import AutoNHITS, AutoLSTM\n",
    "from neuralforecast.losses.pytorch import MAE,RMSE,MQLoss\n",
    "\n",
    "from utilsforecast.plotting import plot_series\n",
    "from utilsforecast.losses import mse, mae, rmse\n",
    "from utilsforecast.evaluation import evaluate\n",
    "\n",
    "#Model Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score,make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import TimeSeriesSplit,GridSearchCV,ParameterGrid \n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c52a22",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(r\"/mnt/c/Users/aimeraj/Desktop/data/train.csv\")\n",
    "stores_set = pd.read_csv(r\"/mnt/c/Users/aimeraj/Desktop/data/stores.csv\")\n",
    "oil_set = pd.read_csv(r\"/mnt/c/Users/aimeraj/Desktop/data/oil.csv\")\n",
    "holiday_set = pd.read_csv(r\"/mnt/c/Users/aimeraj/Desktop/data/holidays_events.csv\")\n",
    "transactions_set =pd.read_csv(r\"/mnt/c/Users/aimeraj/Desktop/data/transactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54e57b",
   "metadata": {},
   "source": [
    "#### Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "#αν θελω reset\n",
    "#pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29a87e",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ελεγχος για διπλοτυπα\n",
    "print(train_set.duplicated().sum(),\n",
    "      stores_set.duplicated().sum(),\n",
    "      oil_set.duplicated().sum(),\n",
    "      holiday_set.duplicated().sum(),\n",
    "      transactions_set.duplicated().sum()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e87363",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce62852",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf743c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769329ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa075df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_set.locale.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merges\n",
    "df = pd.merge(train_set,stores_set,on='store_nbr',how='left')\n",
    "df = pd.merge(df,oil_set,on ='date',how='left')\n",
    "df = pd.merge(df,transactions_set,on=['date','store_nbr'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The dataset contains {df.shape[0]} samples and \"\n",
    "    f\"{df.shape[1]} features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c43f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Κοιταω για Nas\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ea4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_zero = df[df['sales'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ea617",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_zero.isna().sum() # 291028 - 245869 = 45159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sales']==0,'transactions']=0\n",
    "del sales_zero\n",
    "df['transactions'] = df.groupby('store_nbr')['transactions'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Απο αυτο βλεπουμε οτι δεν εχουμε την τιμη του πετρελαιου για ολες τις ημερομινιες\n",
    "print(len(train_set['date'].unique()),len(oil_set['date'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date']= pd.to_datetime(df['date'])\n",
    "df['day']= df['date'].dt.day_name()\n",
    "working_days = df[df['date'].dt.dayofweek < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df['day'] == 'Saturday', 'dcoilwtico'].isna().sum() +\n",
    "      df.loc[df['day'] == 'Sunday', 'dcoilwtico'].isna().sum() \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c028ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Βρισκω το Σ/Κ με τιμες Na στο πετρελαιο\n",
    "weekend_mask = df['day'].isin(['Saturday', 'Sunday']) & df['dcoilwtico'].isna()\n",
    "df['price_filled'] = df['dcoilwtico']\n",
    "df['price_filled'] = df['price_filled'].where(df['day'].isin(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']))\n",
    "df['price_filled'] = df['price_filled'].ffill()\n",
    "df.loc[weekend_mask, 'dcoilwtico'] = df.loc[weekend_mask, 'price_filled']\n",
    "df.drop(columns=['price_filled'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819335ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dcoilwtico'] = df['dcoilwtico'].interpolate(method='linear')\n",
    "df['dcoilwtico'] = df['dcoilwtico'].bfill()\n",
    "df['transactions'] = df['transactions'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Για τις υπολοιπες τιμες του πετρελαιου που δεν εχουμε κανουμε imputation\n",
    "#df['dcoilwtico'] = df['dcoilwtico'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d832ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ec89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f81f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e435244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479faf37",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cce771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The dataset contains {df.shape[0]} samples and \"\n",
    "    f\"{df.shape[1]} features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b475c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['store_nbr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8509e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['family'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a971fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['family'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20338ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['city'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be766ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87407a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sales','onpromotion','transactions','dcoilwtico']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Εδω δημιουργω ενα γραφημα για τα sales σε βαθος χρονου\n",
    "#Για να γινει αυτο θα πρεπει να αθροισω τα sales καθε μερας καθως το υπαρχον dataset εχει \n",
    "#sales για ολες τις ημερες \n",
    "daily_sales = df[['date','sales']].copy()\n",
    "daily_sales = daily_sales.groupby('date').sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(daily_sales,x='date',y='sales',title=\"Daily Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Εδω ενονω τις τιμες του πετρελαιου με τα daily sales για να κανω correlation test\n",
    "daily_oil = df[['date', 'dcoilwtico']].dropna().drop_duplicates(subset='date')\n",
    "daily_df = pd.merge(daily_sales,daily_oil,on='date',how='left')\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average = daily_df['sales'].rolling(\n",
    "    window = len(daily_df['sales']),\n",
    "    center = True,\n",
    "    min_periods = len(daily_df['sales'])//2,\n",
    ").mean()\n",
    "\n",
    "daily_df['moving_average'] = moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.scatter(daily_df, x='date', y='sales')\n",
    "fig2 = px.line(daily_df, x='date', y='moving_average', color_discrete_sequence=['red'])\n",
    "fig3 = go.Figure(data=fig1.data + fig2.data)\n",
    "fig3 = fig3.update_layout(xaxis_title=\"Sales\", yaxis_title=\"Date\", title = 'Moving Average Over Time')\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['date'].dt.year\n",
    "avg_sales_year_family = df.groupby(['year', 'family'])['sales'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad59fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.line(\n",
    "    avg_sales_year_family,\n",
    "    x='year',\n",
    "    y='sales',\n",
    "    color='family',\n",
    "    facet_col='family',\n",
    "    facet_col_wrap= 4,\n",
    "    markers=True,\n",
    "    title='Average Yearly Sales per Product Family'\n",
    ")\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.update_layout(height=2000,showlegend=False)\n",
    "\n",
    "# fig.update_layout(\n",
    "#     xaxis_title='Year',\n",
    "#     yaxis_title='Average Sales',\n",
    "#     legend_title='Family',\n",
    "#     width=1000,\n",
    "#     height=600\n",
    "# )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ffd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].dt.month\n",
    "df['month_name'] = df['date'].dt.strftime('%B')\n",
    "avg_sales_by_month = df.groupby(['month', 'month_name', 'family'])['sales'].mean().reset_index()\n",
    "\n",
    "avg_sales_by_month = avg_sales_by_month.sort_values('month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d20e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    avg_sales_by_month,\n",
    "    x='month_name',\n",
    "    y='sales',\n",
    "    color='family',\n",
    "    facet_col='family',\n",
    "    facet_col_spacing=0.1,\n",
    "    facet_col_wrap=4,\n",
    "    markers=True,\n",
    "    category_orders={\"month_name\": [\n",
    "        \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "        \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n",
    "    ]},\n",
    "    title='Average Sales by Calendar Month per Product Family'\n",
    ")\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.update_layout(height=2000,showlegend=False)\n",
    "\n",
    "# fig.update_layout(\n",
    "#     xaxis_title='Month',\n",
    "#     yaxis_title='Average Sales',\n",
    "#     legend_title='Family',\n",
    "#     width=1000,\n",
    "#     height=600\n",
    "# )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_nbr'] = df['date'].dt.weekday\n",
    "avg_sales_by_weekday = df.groupby(['day_nbr', 'day', 'family'])['sales'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4169ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_by_weekday = avg_sales_by_weekday.sort_values('day_nbr')\n",
    "fig = px.line(\n",
    "    avg_sales_by_weekday,\n",
    "    x='day',\n",
    "    y='sales',\n",
    "    color='family',\n",
    "    facet_col='family',\n",
    "    facet_col_wrap=4,\n",
    "    markers=True,\n",
    "    category_orders={\"day\": [\n",
    "        \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"\n",
    "    ]},\n",
    "    title='Average Sales by Day of Week per Product Family'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.update_layout(height=2000,showlegend=False)\n",
    "\n",
    "# fig.update_layout(\n",
    "#     xaxis_title='Day of Week',\n",
    "#     yaxis_title='Average Sales',\n",
    "#     legend_title='Family',\n",
    "#     width=1000,\n",
    "#     height=600\n",
    "# )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ad917",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(\"family\")['sales'].mean().sort_values(ascending=False).reset_index()\n",
    "px.bar(t,y='family',x='sales',color='family',title='Average sales per product family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5476ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(\"city\")['sales'].mean().sort_values(ascending=False).reset_index()\n",
    "px.bar(t,y='city',x='sales',color='city',title='Average sales per City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = df[[\"store_nbr\", \"sales\"]].copy()\n",
    "# temp[\"ind\"] = 1\n",
    "# temp[\"ind\"] = temp.groupby(\"store_nbr\")[\"ind\"].cumsum().values\n",
    "# pivot_sales = pd.pivot(temp, index=\"ind\", columns=\"store_nbr\", values=\"sales\")\n",
    "\n",
    "# corr_matrix = pivot_sales.corr()\n",
    "# mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# sns.heatmap(corr_matrix,\n",
    "#             annot=True,\n",
    "#             fmt='.1f',\n",
    "#             cmap='coolwarm',\n",
    "#             square=True,\n",
    "#             mask=mask,\n",
    "#             linewidths=1,\n",
    "#             cbar=False)\n",
    "# plt.title(\"Correlations among stores\", fontsize=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526bd02e",
   "metadata": {},
   "source": [
    "#### Oil Price - Impact on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Αυτο εχει και αλλη δουλεια\n",
    "px.line(daily_oil,x='date',y='dcoilwtico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(daily_df['sales'],bins=30, color='skyblue', edgecolor='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(daily_df['dcoilwtico'],bins=30, color='skyblue', edgecolor='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a909018",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, p_value = spearmanr(daily_df['sales'], daily_df['dcoilwtico'])\n",
    "\n",
    "print(f\"Spearman correlation: {corr}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(daily_df['dcoilwtico'], daily_df['sales'], alpha=0.5)\n",
    "plt.title('Sales ~ Dcoilwtico')\n",
    "plt.xlabel('dcoilwtico')\n",
    "plt.ylabel('sales')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb1577",
   "metadata": {},
   "source": [
    "#### OnPromotion - Impact on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, p_value = spearmanr(df['sales'], df['onpromotion'])\n",
    "\n",
    "print(f\"Spearman correlation: {corr}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['onpromotion'],df['sales'], alpha=0.5)\n",
    "plt.title('Sales ~ Onpromotion')\n",
    "plt.xlabel('Onpromotion')\n",
    "plt.ylabel('sales')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965811cc",
   "metadata": {},
   "source": [
    "#### Earthquake - Impact on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df[\"date\"] = pd.to_datetime(daily_df[\"date\"])\n",
    "before = daily_df[(daily_df[\"date\"] >= '2016-03-01') & (daily_df[\"date\"] <= '2016-04-15')]\n",
    "after = daily_df[(daily_df[\"date\"] >= '2016-04-16') & (daily_df[\"date\"] <= '2016-05-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(before['sales'], bins=30, color='skyblue', edgecolor='black');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49575a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(after['sales'], bins=30, color='skyblue', edgecolor='black');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p_value = mannwhitneyu(before['sales'], after['sales'])\n",
    "\n",
    "print(f\"U statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "before[['sales']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "after[['sales']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sales']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325588aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del avg_sales_by_month,avg_sales_by_weekday,avg_sales_year_family,moving_average,daily_df,daily_oil,daily_sales\n",
    "del train_set,oil_set,transactions_set,stores_set,before,after,weekend_mask\n",
    "del fig,fig1,fig2,fig3,working_days,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_set['date'] = pd.to_datetime(holiday_set['date'])\n",
    "holiday_set[holiday_set['transferred']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671efe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Το προβλημα με τα transfers\n",
    "temp1 = holiday_set[(holiday_set['type']=='Holiday')& (holiday_set['transferred']==True)].drop('transferred',axis=1).reset_index(drop=True)\n",
    "temp2 = holiday_set[(holiday_set['type']=='Transfer')].drop('transferred',axis=1).reset_index(drop=True)\n",
    "temp = pd.concat([temp1,temp2],axis=1)\n",
    "temp = temp.iloc[:,[5,1,2,3,4,]]\n",
    "holiday_set = holiday_set[(holiday_set['type']!='Transfer')& (holiday_set['transferred']==False)].drop('transferred',axis=1)\n",
    "holiday_set = pd.concat([holiday_set, temp]).reset_index(drop=True)\n",
    "del temp,temp1,temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef330a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Κραταω αυτο για το Prophet μετα\n",
    "holidays_for_prophet = holiday_set.copy()\n",
    "holidays_for_prophet= holidays_for_prophet.drop(['type','locale_name','locale'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384680a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_set['type']= np.where(holiday_set['type']=='Additional','Holiday',holiday_set['type'])\n",
    "holiday_set['type']= np.where(holiday_set['type']=='Bridge','Holiday',holiday_set['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403080cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_days_left = holiday_set[holiday_set.type=='Work Day']\n",
    "holiday_set = holiday_set[holiday_set.type!='Work Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805df90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = holiday_set[holiday_set.type=='Event'].drop(['type','locale','locale_name'],axis=1)\n",
    "events= events.rename({\"description\":\"event\"},axis=1)\n",
    "holiday_set = holiday_set[holiday_set.type!='Event'].drop(\"type\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regional = holiday_set[holiday_set['locale']=='Regional'].rename({\"locale_name\":\"state\",\"description\":\"holiday_regional\"},axis=1).drop(\"locale\",axis=1).drop_duplicates()\n",
    "national = holiday_set[holiday_set['locale']=='National'].rename({\"description\":\"holiday_national\"},axis=1).drop([\"locale\",\"locale_name\"],axis=1).drop_duplicates()\n",
    "local = holiday_set[holiday_set['locale']=='Local'].rename({\"locale_name\":\"city\",\"description\":\"holiday_local\"},axis=1).drop(\"locale\",axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "events['date'] = pd.to_datetime(events['date'])\n",
    "national['date'] = pd.to_datetime(national['date'])\n",
    "regional['date'] = pd.to_datetime(regional['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events[events['date'].duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd483fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(national[national['date'].duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78695a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local[local['date'].duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2aa4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate keys in the lookup tables\n",
    "print(national['date'].duplicated().sum())         # Should be 0\n",
    "print(regional.duplicated(['date', 'state']).sum()) # Should be 0\n",
    "print(local.duplicated(['date', 'city']).sum())     # Should be 0\n",
    "print(events['date'].duplicated().sum())           # Might not be 0 if multiple events per date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e886c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate keys in the lookup tables\n",
    "print(national['date'].duplicated().sum())         # Should be 0\n",
    "print(regional.duplicated(['date', 'state']).sum()) # Should be 0\n",
    "print(local.duplicated(['date', 'city']).sum())     # Should be 0\n",
    "print(events['date'].duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897428a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events.drop_duplicates(subset=['date'])\n",
    "national= national.drop_duplicates(subset=['date'])\n",
    "local= local.drop_duplicates(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events['date'] = pd.to_datetime(events['date'])\n",
    "national['date'] = pd.to_datetime(national['date'])\n",
    "regional['date'] = pd.to_datetime(regional['date'])\n",
    "df = pd.merge(df,national,how='left',on='date')\n",
    "df = pd.merge(df,regional,how='left',on=['date',\"state\"])\n",
    "df = pd.merge(df,local,how='left',on=['date',\"city\"])\n",
    "df = pd.merge(df,events,on=['date'],how='left')\n",
    "del national,regional,local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure 'date' column is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Generate complete date range from min to max\n",
    "full_range = pd.date_range(start=df['date'].min(), end=df['date'].max())\n",
    "\n",
    "# Get unique dates from your data\n",
    "present_dates = df['date'].drop_duplicates()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = full_range.difference(present_dates)\n",
    "\n",
    "# Output missing dates\n",
    "print(\"Missing dates:\")\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8722ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = pd.DataFrame({'date': missing_dates})\n",
    "store_family_combos = df[['store_nbr', 'family', 'city', 'state', 'cluster','type']].drop_duplicates()\n",
    "missing_df = pd.DataFrame([(d, *row) for d in missing_dates for row in store_family_combos.itertuples(index=False)],\n",
    "                          columns=['date', 'store_nbr', 'family', 'city', 'state', 'cluster','type'])\n",
    "missing_df['id'] = range(3000888, 3000888 + len(missing_df))\n",
    "missing_df['sales'] = 0\n",
    "missing_df['onpromotion'] = 0\n",
    "missing_df['transactions'] = 0\n",
    "missing_df['day'] = missing_df['date'].dt.day\n",
    "missing_df['month'] = missing_df['date'].dt.month\n",
    "missing_df['year'] = missing_df['date'].dt.year\n",
    "missing_df['month_name'] = missing_df['date'].dt.month_name()\n",
    "missing_df['day_nbr'] = missing_df['date'].dt.weekday\n",
    "missing_df['holiday_national'] = 'no_holiday'\n",
    "missing_df['holiday_regional'] = 'no_holiday'\n",
    "missing_df['holiday_local'] = 'no_holiday'\n",
    "missing_df['event'] = 'Christmas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d046079",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2013-12-24', '2014-12-24', '2015-12-24', '2016-12-24']\n",
    "df[df['date'].isin(pd.to_datetime(dates))].drop_duplicates(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0805713",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df['dcoilwtico'] = 0\n",
    "missing_df['dcoilwtico'] = missing_df['dcoilwtico'].astype(float)\n",
    "missing_df.loc[missing_df['date'] == '2013-12-25', 'dcoilwtico'] = 98.87\n",
    "missing_df.loc[missing_df['date'] == '2014-12-25', 'dcoilwtico'] = 55.70\n",
    "missing_df.loc[missing_df['date'] == '2015-12-25', 'dcoilwtico'] = 37.62\n",
    "missing_df.loc[missing_df['date'] == '2016-12-25', 'dcoilwtico'] = 52.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([df, missing_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9913490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"holiday_national_binary\"] = np.where(df[\"holiday_national\"].notnull(),1,0)\n",
    "df[\"holiday_regional_binary\"] = np.where(df[\"holiday_regional\"].notnull(),1,0)\n",
    "df[\"holiday_local_binary\"] = np.where(df[\"holiday_local\"].notnull(),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_events= pd.get_dummies(events,\"event\",dtype=int)\n",
    "encoded_events.columns=encoded_events.columns.str.replace(\" \",\"_\")\n",
    "encoded_events.columns=encoded_events.columns.str.replace(\":\",\"_\")\n",
    "encoded_events.columns=encoded_events.columns.str.replace(\"+\",\"_\")\n",
    "encoded_events.columns=encoded_events.columns.str.replace(\"-\",\"_\")\n",
    "df = pd.merge(df,encoded_events,how='left',on='date')\n",
    "del encoded_events,events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13734eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event'] = df['event'].fillna('no_event')\n",
    "df['holiday_national'] = df['holiday_national'].fillna('no_holiday')\n",
    "df['holiday_regional'] = df['holiday_regional'].fillna('no_holiday')\n",
    "df['holiday_local'] = df['holiday_local'].fillna('no_holiday')\n",
    "event_cols = [col for col in df.columns if col.startswith('event')]\n",
    "event_cols.remove('event')\n",
    "df[event_cols] = df[event_cols].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def AB_Test(d,group,target):\n",
    "#     groupA = d[d[group] == 1][target]\n",
    "#     groupB = d[d[group] == 0][target]\n",
    "#     if(len(groupA)<50):\n",
    "#         t1 = shapiro(groupA)[1] < 0.05\n",
    "#     else:\n",
    "#         t1 = lilliefors(groupA)[1] < 0.05\n",
    "    \n",
    "#     if(len(groupB)<50):\n",
    "#         t2 = shapiro(groupB)[1] < 0.05\n",
    "#     else:\n",
    "#         t2 = lilliefors(groupB)[1] < 0.05\n",
    "    \n",
    "#     if (t1 == False) & (t1 == False):\n",
    "#         leveneTest = stats.levene(groupA, groupB)[1] < 0.05\n",
    "        \n",
    "#         if leveneTest == False:\n",
    "#             ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n",
    "\n",
    "#         else:\n",
    "#             ttest = stats.wilcoxon(groupA, groupB, equal_var=False)[1]\n",
    "\n",
    "#     else:\n",
    "#         ttest = stats.mannwhitneyu(groupA, groupB)[1] \n",
    "\n",
    "#     temp = pd.DataFrame({\n",
    "#         \"variable\":[group],\n",
    "#         \"p-value\":[ttest]\n",
    "#     })\n",
    "#     return temp\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for i in ['holiday_national_binary','holiday_regional_binary','holiday_local_binary']:\n",
    "#     results.append(AB_Test(d=df[df['sales'].notnull()],group=i,target='sales'))\n",
    "\n",
    "# results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a194b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c697f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[]\n",
    "# for i in event_cols:\n",
    "#     results.append(AB_Test(d=df[df['sales'].notnull()],group=i,target='sales'))\n",
    "# results = pd.concat(results)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.loc[results['p-value']>0.05,]\n",
    "# del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unique_id'] = df['store_nbr'].astype(str) + '_' + df['family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c179cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero forecasting\n",
    "zero_sales = df[['unique_id','sales']].copy()\n",
    "zero_sales=zero_sales.groupby(['unique_id']).sum().reset_index()\n",
    "zero_sales=zero_sales[zero_sales['sales']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa51c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566100f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_sales.shape # βαση αυτου υπαρχουν 53 combos που εχουν 0 πωλησεις"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d676352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_sales_by_day = df[df['sales'] == 0].groupby('day').size()\n",
    "# total_by_day = df.groupby('day').size()\n",
    "# zero_sales_rate = zero_sales_by_day / total_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3774814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_sales_by_day.plot(kind='bar', title='Number of Zero Sales by Day of Week')\n",
    "# plt.xlabel('Day of Week')\n",
    "# plt.ylabel('Number of Zero Sales')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6751b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_per_uid = df.groupby('unique_id')['sales'].mean().reset_index()\n",
    "avg_sales_per_uid.columns = ['unique_id', 'avg_sales']\n",
    "avg_sales_per_uid.sort_values(by='avg_sales', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_per_uid[avg_sales_per_uid['unique_id'] == '1_CLEANING']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_sales = df[['unique_id','sales']].copy()\n",
    "zero_sales=zero_sales.groupby(['unique_id']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_sales.sales.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebcd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['store_nbr', 'family', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eba652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb9e55",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA #44_GROCERY I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29517476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extract Series\n",
    "\n",
    "my_series = df[df['unique_id'] == '1_CLEANING'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_series = my_series.drop([\n",
    "    'id',\n",
    "    'store_nbr',\n",
    "    'family',\n",
    "    'onpromotion',\n",
    "    'city',\n",
    "    'state',\n",
    "    'type',\n",
    "    'cluster',\n",
    "    'dcoilwtico',\n",
    "    'transactions',\n",
    "    'day',\n",
    "    'year',\n",
    "    # 'month',\n",
    "    'month_name',\n",
    "    # 'day_nbr', \n",
    "    'holiday_national',\n",
    "    'holiday_regional',\n",
    "    'holiday_local', \n",
    "    'event',\n",
    "    'unique_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_series.set_index('date', inplace=True)\n",
    "my_series = my_series.asfreq('D') \n",
    "my_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c271c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(my_series['sales'],period=7)\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(my_series['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56bdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima(my_series['sales'],seasonal=True,m=30,trace=True,error_action='ignore',suppress_warnings=True,stepwise=True).summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_start_date = '2017-07-17'\n",
    "# train_arima = my_series[my_series.index < forecast_start_date]\n",
    "# test_arima = my_series[my_series.index >= forecast_start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d31b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exog_train = train_arima.drop(columns=['sales'])\n",
    "# exog_test = test_arima.drop(columns=['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ea64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SARIMAX(np.log1p(train_arima['sales']),exog=exog_train,order=(2,1,0),sesonal_order=(1,0,1,7),enforce_invertibility=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model = SARIMAX(train_arima['sales'],exog=exog_train,order=(2,1,2),seasonal_order=(0,0,1,30),enforce_invertibility=False) #44_GROCERY I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.fit()\n",
    "# results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3bed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = results.predict(start=start, end=end, exog=exog_forecast).rename('SARIMAX(5,1,2)(2,0,0,14) Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = np.expm1(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtain predicted values\n",
    "# start=len(train_arima)\n",
    "# end=len(train_arima)+len(test_arima)-1\n",
    "# exog_forecast = exog_test\n",
    "# predictions = results.predict(start=start, end=end, exog=exog_forecast).rename('SARIMAX(5,1,2)(2,0,0,14) Predictions')\n",
    "# pred = np.expm1(predictions)\n",
    "\n",
    "\n",
    "# # Plot predictions against known values\n",
    "# title='Sales of 1_BREAD/BAKERY'\n",
    "# ylabel='Sales per day'\n",
    "# xlabel=''\n",
    "\n",
    "# ax = test_arima['sales'].plot(legend=True,figsize=(12,6),title=title)\n",
    "# pred.plot(legend=True)\n",
    "# ax.autoscale(axis='x',tight=True)\n",
    "# ax.set(xlabel=xlabel, ylabel=ylabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5af4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error1x = mean_squared_error(test_arima['sales'], pred)\n",
    "# error2x = np.sqrt(error1x)\n",
    "# # Print new SARIMAX values\n",
    "# print(f'SARIMAX(5,1,2)(1,0,0,30) R2: {r2_score(test_arima['sales'], pred)}')\n",
    "# print(f'SARIMAX(5,1,2)(1,0,0,30) MSE Error: {error1x:11.10}')\n",
    "# print(f'SARIMAX(5,1,2)(1,0,0,30) RMSE Error: {error2x:11.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SARIMAX(train_arima['sales'],exog=exog_train[['holiday_national_binary','event_Terremoto_Manabi_1','event_Terremoto_Manabi_14']],order=(2,1,2),seasonal_order=(0,0,1,30),enforce_invertibility=False)\n",
    "# results = model.fit()\n",
    "# results.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific = ['holiday_national_binary','holiday_regional_binary','event_Dia_de_la_Madre','day_nbr']\n",
    "# model = SARIMAX(train_arima['sales'],exog=exog_train[specific],order=(5,1,2),sesonal_order=(1,0,0,30),enforce_invertibility=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.fit()\n",
    "# results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd94c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exog_test[['holiday_national_binary','holiday_regional_binary','event_Dia_de_la_Madre','day_nbr']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtain predicted values\n",
    "# start=len(train_arima)\n",
    "# end=len(train_arima)+len(test_arima)-1\n",
    "# exog_forecast = exog_test[['holiday_national_binary','holiday_regional_binary','event_Dia_de_la_Madre','day_nbr']].copy()\n",
    "# predictions = results.predict(start=start, end=end, exog=exog_forecast).rename('SARIMAX(2,1,2)(0,0,1,30) Predictions')\n",
    "\n",
    "# # Plot predictions against known values\n",
    "# title='Sales of 44_GROCERY I'\n",
    "# ylabel='Sales per day'\n",
    "# xlabel=''\n",
    "\n",
    "# ax = test_arima['sales'].plot(legend=True,figsize=(12,6),title=title)\n",
    "# predictions.plot(legend=True)\n",
    "# ax.autoscale(axis='x',tight=True)\n",
    "# ax.set(xlabel=xlabel, ylabel=ylabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error1x = mean_squared_error(test_arima['sales'], predictions)\n",
    "# error2x = np.sqrt(error1x)\n",
    "\n",
    "# # Print new SARIMAX values\n",
    "# print(f'SARIMAX(2,1,2)(0,0,1,30) MSE Error: {error1x:11.10}')\n",
    "# print(f'SARIMAX(2,1,2)(0,0,1,30) RMSE Error: {error2x:11.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365746a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you're working with a time series like:\n",
    "# series = train_arima['sales']\n",
    "\n",
    "# # Plot ACF\n",
    "# plot_acf(series, lags=40)\n",
    "# plt.title(\"ACF Plot\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot PACF\n",
    "# plot_pacf(series, lags=40, method='ywm')  # 'ywm' is preferred for stability\n",
    "# plt.title(\"PACF Plot\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['lag_1'] = df.groupby(['store_nbr','family'])['sales'].shift(1)\n",
    "df['lag_7'] = df.groupby(['store_nbr','family'])['sales'].shift(7)\n",
    "df['lag_14'] = df.groupby(['store_nbr','family'])['sales'].shift(14)\n",
    "df['lag_28'] = df.groupby(['store_nbr','family'])['sales'].shift(28)\n",
    "df['rolling_mean_7'] = df.groupby(['store_nbr','family'])['sales'].transform(lambda x: x.shift(1).rolling(7).mean())\n",
    "df['rolling_mean_14'] = df.groupby(['store_nbr','family'])['sales'].transform(lambda x: x.shift(1).rolling(14).mean())\n",
    "df['rolling_std_7'] = df.groupby(['store_nbr','family'])['sales'].transform(lambda x: x.shift(1).rolling(7).std())\n",
    "df['oil_lag_1'] = df.groupby(['store_nbr','family'])['dcoilwtico'].shift(1)\n",
    "df['oil_lag_7'] = df.groupby(['store_nbr','family'])['dcoilwtico'].shift(7)\n",
    "df['oil_lag_14'] = df.groupby(['store_nbr','family'])['dcoilwtico'].shift(14)\n",
    "df= df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df.groupby(['date','unique_id']).agg({'sales':'sum'}).reset_index().sort_values(['unique_id','date'])\n",
    "total_sales_df = agg_df.pivot(index='date',columns='unique_id', values='sales')\n",
    "agg_promotions_df = df.groupby(['date','unique_id']).agg({'onpromotion':'sum'}).reset_index().sort_values(['unique_id','date'])\n",
    "total_promotions_df = agg_promotions_df.pivot(index='date',columns='unique_id', values='onpromotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81219aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to break out each into it's own dataframe for prediction since each will have different rows affected\n",
    "prediction_df_list = []\n",
    "\n",
    "#Cleaning up dataframe using z-score to remove outliers which heavily bias the model\n",
    "for column in total_sales_df.columns:\n",
    "    df_clean = total_sales_df[[column]].reset_index()\n",
    "    \n",
    "    z = np.abs(stats.zscore(df_clean[column]))\n",
    "    outlier_index = np.where(z > 2.7)[0] #As 99.7% of the data points lie between 3 standard deviations (Gaussian Distribution)\n",
    "    print(\"Dropping \"+str(len(outlier_index))+\" rows for following category: \"+column)\n",
    "    df_clean.drop(index=outlier_index,inplace=True)\n",
    "    df_clean.set_index('date', inplace=True)\n",
    "    prediction_df_list.append(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753186e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changepoint_prior_scale_range = np.linspace(0.001, 0.5, num=5).tolist()\n",
    "# seasonality_prior_scale_range = np.linspace(0.01, 10, num=5).tolist()\n",
    "# holidays_prior_scale_range = np.linspace(0.01, 10, num=5).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(total_sales_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e567c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_mape(y_true, y_pred):\n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "#     non_zero = y_true != 0\n",
    "#     return np.mean(np.abs((y_true[non_zero] - y_pred[non_zero]) / y_true[non_zero])) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_params_Prophet(feature='1_AUTOMOTIVE'):\n",
    "# # for feature in total_sales_df.columns[330:331]:\n",
    "  \n",
    "#     category_df = total_sales_df[feature].copy().reset_index()\n",
    "#     category_df.columns = [\"ds\", \"y\"]\n",
    "\n",
    "#     category_df[[\"y\"]] = category_df[[\"y\"]].apply(pd.to_numeric)\n",
    "#     category_df[\"ds\"] = pd.to_datetime(category_df[\"ds\"])\n",
    "    \n",
    "#     param_grid = {  \n",
    "#         \"changepoint_prior_scale\": changepoint_prior_scale_range,\n",
    "#         \"seasonality_prior_scale\": seasonality_prior_scale_range,}\n",
    "\n",
    "#     # Generate all combinations of parameters\n",
    "#     all_params = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
    "#     rmses = [] \n",
    "\n",
    "#     # Use cross validation to evaluate all parameters\n",
    "#     for params in all_params:\n",
    "#         m = Prophet(**params).fit(category_df)  # Fit model with given params\n",
    "#         df_cv = cross_validation(m, initial=\"365 days\", period=\"30 days\", horizon = \"30 days\")\n",
    "#         df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "#         print(df_p)\n",
    "#         rmses.append(df_p[\"rmse\"].values[0])\n",
    "\n",
    "#     # Find the best parameters\n",
    "#     tuning_results = pd.DataFrame(all_params)\n",
    "#     tuning_results[\"rmse\"] = rmses\n",
    "    \n",
    "#     print(feature)\n",
    "#     print(tuning_results.head())\n",
    "\n",
    "#     params_dict = dict(tuning_results.sort_values(\"rmse\").reset_index(drop=True).iloc[0])\n",
    "#     params_dict[\"column\"] = feature \n",
    "    \n",
    "#     return params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=find_params_Prophet('1_CLEANING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_days = 30\n",
    "# forecast_start_date = '2017-07-17'\n",
    "# def make_preds_Prophet(feature='1_AUTOMOTIVE',dicts={}):\n",
    "#     category_df = total_sales_df[feature].copy().reset_index()\n",
    "#     category_df.columns = ['date','y']\n",
    "#     category_df[['y']] = category_df[['y']].apply(pd.to_numeric)\n",
    "#     category_df[\"date\"] = pd.to_datetime(category_df[\"date\"])\n",
    "\n",
    "#     temp = df[df['unique_id'] == feature].copy()\n",
    "\n",
    "#     temp = temp.drop(['id', 'store_nbr', 'family',\n",
    "#         'sales','city','state', 'type', 'cluster',\n",
    "#         'dcoilwtico', 'transactions','day','month_name',\n",
    "#         'holiday_national','holiday_regional',\n",
    "#         'holiday_local', 'event','unique_id','year','month','day_nbr',\n",
    "#         'holiday_national_binary','holiday_regional_binary','holiday_local_binary'],axis=1)\n",
    "#     temp = temp.drop_duplicates()\n",
    "#     temp = temp.drop_duplicates(subset='date')\n",
    "\n",
    "#     category_df[\"date\"] = pd.to_datetime(category_df[\"date\"])\n",
    "#     category_df= pd.merge(category_df,temp,on='date',how='left')\n",
    "#     category_df=category_df.drop_duplicates()\n",
    "#     category_df= category_df.drop_duplicates(subset=['date', 'y'])\n",
    "#     category_df= category_df.rename({\"date\":\"ds\"},axis=1)\n",
    "#     train_data = category_df[category_df['ds'] < forecast_start_date]\n",
    "#     test_data = category_df[category_df['ds'] >= forecast_start_date]\n",
    "#     test_data = test_data.drop(['y'],axis=1)\n",
    "    \n",
    "#     #Holidays for each unique id & formating\n",
    "#     holiday_rows = df[\n",
    "#         (df['unique_id'] == feature) & (\n",
    "#         (df['holiday_national_binary'] == 1) |\n",
    "#         (df['holiday_regional_binary'] == 1) |\n",
    "#         (df['holiday_local_binary'] == 1))\n",
    "#     ].copy()\n",
    "#     dates = holiday_rows[['date']].copy()\n",
    "#     holidays_df = dates.merge(holidays_for_prophet, on='date', how='left').copy()\n",
    "#     holidays_df= holidays_for_prophet.rename(columns={'date': 'ds','description': 'holiday'})\n",
    "    \n",
    "#     #model\n",
    "#     m = Prophet(changepoint_prior_scale = dicts['changepoint_prior_scale'],\n",
    "#                 seasonality_prior_scale = dicts['seasonality_prior_scale'],\n",
    "#                 seasonality_mode = 'multiplicative',\n",
    "#                 holidays=holidays_df\n",
    "#                 )\n",
    "#     # Add regressors\n",
    "#     regressors = category_df.columns.tolist()\n",
    "#     regressors.remove('ds') ; regressors.remove('y')\n",
    "#     # regressors.remove('onpromotion')\n",
    "#     for regressor in regressors:\n",
    "#         m.add_regressor(regressor)\n",
    "\n",
    "#     m.fit(train_data)\n",
    "\n",
    "#     future = m.make_future_dataframe(periods=prediction_days)\n",
    "#     future = pd.merge(future,category_df,on='ds',how='left')\n",
    "\n",
    "#     fcst_prophet_train = m.predict(future)\n",
    "#     filter = fcst_prophet_train['ds']>=forecast_start_date \n",
    "#     predicted_df = fcst_prophet_train[filter][['ds','yhat']]\n",
    "#     predicted_df = predicted_df.merge(category_df)\n",
    "\n",
    "#     rmsle= np.sqrt(mean_squared_log_error(predicted_df['y'],predicted_df['yhat']))\n",
    "#     rmse = np.sqrt(mean_squared_error(predicted_df['y'], predicted_df['yhat']))\n",
    "#     r2 = r2_score(predicted_df['y'],predicted_df['yhat'])\n",
    "#     mape = calculate_mape(predicted_df['y'].values,predicted_df['yhat'].values)\n",
    "\n",
    "#     print(feature,rmsle)\n",
    "#     print(feature,rmse)\n",
    "#     print(feature,r2)\n",
    "#     print(mape)\n",
    "#     df_plot = pd.DataFrame({\n",
    "#         'date': predicted_df['ds'].unique(),\n",
    "#         'actual': predicted_df['y'].values,\n",
    "#         'predicted': predicted_df['yhat'].values,\n",
    "#         })\n",
    "\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(df_plot['date'], df_plot['actual'], label='Actual Sales')\n",
    "#     plt.plot(df_plot['date'], df_plot['predicted'], label='Predictided Sales')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Sales')\n",
    "#     plt.title('Comparison of Actual and Predictided Sales')\n",
    "#     plt.legend()\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     return feature, m, train_data, test_data, fcst_prophet_train, rmsle,rmse,r2,mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05334110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_params_Prophet('1_CLEANING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a287f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_preds_Prophet(feature='1_CLEANING',dicts=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e705560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecasted_dfs = []\n",
    "\n",
    "# for feature in total_sales_df.columns:\n",
    "    \n",
    "#     selected_feature = str(feature)\n",
    "#     category_df = total_sales_df[feature].copy().reset_index()\n",
    "#     category_df.columns = ['date','y']\n",
    "#     category_df[['y']] = category_df[['y']].apply(pd.to_numeric)\n",
    "#     category_df[\"date\"] = pd.to_datetime(category_df[\"date\"])\n",
    "\n",
    "#     temp = df[df['unique_id'] == selected_feature].copy()\n",
    "\n",
    "#     temp = temp.drop(['id', 'store_nbr', 'family',\n",
    "#         'sales','city','state', 'type', 'cluster',\n",
    "#         'dcoilwtico', 'transactions','day','month_name',\n",
    "#         'holiday_national','holiday_regional',\n",
    "#         'holiday_local', 'event','unique_id','year','month','day_nbr',\n",
    "#         'holiday_national_binary','holiday_regional_binary','holiday_local_binary'],axis=1)\n",
    "#     temp = temp.drop_duplicates()\n",
    "#     temp = temp.drop_duplicates(subset='date')\n",
    "\n",
    "#     category_df[\"date\"] = pd.to_datetime(category_df[\"date\"])\n",
    "#     category_df= pd.merge(category_df,temp,on='date',how='left')\n",
    "#     category_df=category_df.drop_duplicates()\n",
    "#     category_df= category_df.drop_duplicates(subset=['date', 'y'])\n",
    "#     category_df= category_df.rename({\"date\":\"ds\"},axis=1)\n",
    "#     train_data = category_df[category_df['ds'] < forecast_start_date]\n",
    "#     test_data = category_df[category_df['ds'] >= forecast_start_date]\n",
    "#     test_data = test_data.drop(['y'],axis=1)\n",
    "#     #Holidays for each unique id & formating\n",
    "#     holiday_rows = df[\n",
    "#         (df['unique_id'] == selected_feature) & (\n",
    "#         (df['holiday_national_binary'] == 1) |\n",
    "#         (df['holiday_regional_binary'] == 1) |\n",
    "#         (df['holiday_local_binary'] == 1))\n",
    "#     ].copy()\n",
    "#     dates = holiday_rows[['date']].copy()\n",
    "#     holidays_df = dates.merge(holidays_for_prophet, on='date', how='left').copy()\n",
    "#     holidays_df= holidays_for_prophet.rename(columns={'date': 'ds','description': 'holiday'})\n",
    "    \n",
    "#     #finding the right params_dict for this unique_id\n",
    "#     params_dict = dicts[feature]\n",
    "\n",
    "#     #model\n",
    "#     m = Prophet(changepoint_prior_scale = dicts[feature]['changepoint_prior_scale'],\n",
    "#                 seasonality_prior_scale = dicts[feature]['seasonality_prior_scale'],\n",
    "#                 seasonality_mode = 'multiplicative',\n",
    "#                 holidays=holidays_df\n",
    "#                 )\n",
    "    \n",
    "#     # Add regressors\n",
    "#     regressors = category_df.columns.tolist()\n",
    "#     regressors.remove('ds') ; regressors.remove('y')\n",
    "#     regressors.remove('onpromotion')\n",
    "\n",
    "#     for regressor in regressors:\n",
    "#         m.add_regressor(regressor)\n",
    "\n",
    "#     m.fit(train_data)\n",
    "\n",
    "#     future = m.make_future_dataframe(periods=prediction_days)\n",
    "#     future = pd.merge(future,category_df,on='ds',how='left')\n",
    "#     future[event_cols] = future[event_cols].fillna(0)\n",
    "#     fcst_prophet_train = m.predict(future)\n",
    "    \n",
    "#     fig1 = m.plot(fcst_prophet_train)\n",
    "#     fig2 = m.plot_components(fcst_prophet_train)\n",
    "\n",
    "#     forecasted_df = fcst_prophet_train[fcst_prophet_train['ds']>=forecast_start_date]\n",
    "#     break\n",
    "#     #forecasted_dfs.append(forecasted_df) #οταν αλλαξω το ποσα combos θα δω τοτε θα βαλω αυτο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#END OF PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b899ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8527878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(event_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['date','sales']]\n",
    "features = [\n",
    " 'onpromotion',\n",
    " 'day_nbr',\n",
    " 'year',\n",
    " 'month',\n",
    " 'holiday_national_binary',\n",
    " 'oil_lag_1',\n",
    " 'lag_7',\n",
    " 'lag_14',\n",
    " 'lag_28',\n",
    " 'rolling_mean_7',\n",
    " 'rolling_mean_14',\n",
    " 'rolling_std_7',\n",
    " 'oil_lag_7',\n",
    " 'oil_lag_14',\n",
    " # 'unique_id',\n",
    " #'family',                                  # αυτα ειναι ολα τα features που τεσταρα αλλα τελικα δεν κρατησα\n",
    " #'is_weekend',\n",
    " #'store_nbr',\n",
    " #'city',\n",
    " #'state',\n",
    " #'type',\n",
    " #'cluster',\n",
    " #'quarter',\n",
    " #'holiday_local_binary',\n",
    " #'holiday_regional_binary',\n",
    " #'lag_1',\n",
    " #'trans_lag_1',                                    # Αυτα ηταν lags για τα transactions\n",
    " #'trans_lag_7',\n",
    " #'trans_lag_14'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad971e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning\n",
    "dicts = {}\n",
    "i = 0\n",
    "for feature in total_sales_df.columns[:2]:\n",
    "    selected_feature = str(feature)\n",
    "    category_df = total_sales_df[feature].copy().reset_index()\n",
    "    temp = df[df['unique_id'] == selected_feature].copy()\n",
    "    category_df.columns = [\"date\", \"sales\"]\n",
    "\n",
    "    temp = temp.drop(['id', 'store_nbr', 'family',\n",
    "        'sales','city','state', 'type', 'cluster',\n",
    "        'dcoilwtico', 'transactions','day','month_name',\n",
    "        'holiday_national','holiday_regional',\n",
    "        'holiday_local', 'event','unique_id'],axis=1)\n",
    "\n",
    "    category_df[\"date\"] = pd.to_datetime(category_df[\"date\"])\n",
    "    category_df= pd.merge(category_df,temp,on='date',how='left')\n",
    "    category_df=category_df.drop_duplicates()\n",
    "    category_df= category_df.drop_duplicates(subset=['date', 'sales'])\n",
    "    category_df=category_df.dropna()\n",
    "\n",
    "    y = category_df['sales']\n",
    "    X = category_df[features].copy()\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth':[7],\n",
    "        'num_leaves': [15, 31,64],\n",
    "        'learning_rate': [0.01,0.05, 0.1],\n",
    "        'n_estimators': [1000],\n",
    "        'reg_alpha': [0.0, 0.1],\n",
    "        'reg_lambda': [0.0, 0.1],\n",
    "        'min_child_samples': [20, 50]\n",
    "    }\n",
    "\n",
    "    lgb_reg = lgb.LGBMRegressor()\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=6, test_size=30)\n",
    "    grid = GridSearchCV(estimator=lgb_reg,\n",
    "                        param_grid=param_grid,\n",
    "                        cv=tscv,\n",
    "                        scoring='neg_root_mean_squared_error',\n",
    "                        verbose=0)\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    best_params = grid.best_params_\n",
    "    best_score = -grid.best_score_\n",
    "\n",
    "    best_params_with_score = best_params.copy()\n",
    "    best_params_with_score[\"rmse\"] = best_score\n",
    "    dicts[feature] = best_params_with_score\n",
    "    print(f\"✅-{i}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec32cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "i = 0\n",
    "for feature in total_sales_df.columns[:2]:\n",
    "    selected_feature = str(feature)\n",
    "    category_df = total_sales_df[feature].copy().reset_index()\n",
    "    temp = df[df['unique_id'] == selected_feature].copy()\n",
    "    category_df.columns = [\"date\", \"sales\"]\n",
    "\n",
    "    temp = temp.drop(['id', 'store_nbr', 'family',\n",
    "        'sales','city','state', 'type', 'cluster',\n",
    "        'dcoilwtico', 'transactions','day','month_name',\n",
    "        'holiday_national','holiday_regional',\n",
    "        'holiday_local', 'event','unique_id'],axis=1)\n",
    "\n",
    "    category_df[\"date\"] = pd.to_datetime(category_df[\"date\"])\n",
    "    category_df= pd.merge(category_df,temp,on='date',how='left')\n",
    "    category_df=category_df.drop_duplicates()\n",
    "    category_df= category_df.drop_duplicates(subset=['date', 'sales'])\n",
    "    category_df=category_df.dropna()\n",
    "\n",
    "    y = category_df[['date', 'sales']]\n",
    "    features= category_df.columns.tolist()[2:]\n",
    "\n",
    "    test_start='2017-07-15'\n",
    "    X_train = category_df[category_df['date']<test_start].copy()\n",
    "    y_train = y[y['date']<test_start].copy()\n",
    "\n",
    "    X_val = category_df[category_df['date']>=test_start].copy()\n",
    "    y_val = y[y['date']>=test_start].copy()\n",
    "\n",
    "    y_train = y_train.drop(['date'],axis=1)\n",
    "    y_val = y_val.drop(['date'],axis=1)\n",
    "\n",
    "    X_train = X_train[features]\n",
    "    X_val = X_val[features]\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        max_depth=dicts[feature]['max_depth'],\n",
    "        num_leaves=dicts[feature]['num_leaves'],\n",
    "        learning_rate=dicts[feature]['learning_rate'],\n",
    "        n_estimators=dicts[feature]['n_estimators'],\n",
    "        reg_alpha=dicts[feature]['reg_alpha'],\n",
    "        reg_lambda=dicts[feature]['reg_lambda'],\n",
    "        min_child_samples=dicts[feature]['min_child_samples'],\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, y_pred)))\n",
    "\n",
    "    m=category_df['sales'].mean()\n",
    "    temp1 = pd.DataFrame({\n",
    "                \"unique_id\":[selected_feature],\n",
    "                \"rmse\":[rmse],\n",
    "                \"ymean\": [m],\n",
    "                \"rmsle\":[rmsle],\n",
    "                })\n",
    "    results.append(temp1)\n",
    "    print(f\"✅-{i}\")\n",
    "    i += 1\n",
    "results=pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['date','sales']]\n",
    "features = [\n",
    " 'onpromotion',\n",
    " 'day_nbr',\n",
    " 'year',\n",
    " 'month',\n",
    " 'holiday_national_binary',\n",
    " 'oil_lag_1',\n",
    " 'lag_7',\n",
    " 'lag_14',\n",
    " 'lag_28',\n",
    " 'rolling_mean_7',\n",
    " 'rolling_mean_14',\n",
    " 'rolling_std_7',\n",
    " 'oil_lag_7',\n",
    " 'oil_lag_14',\n",
    " 'unique_id',\n",
    "]\n",
    "X = df[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31506846",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start='2017-07-15'\n",
    "X_train = df[df['date']<test_start].copy()\n",
    "y_train = y[y['date']<test_start].copy()\n",
    "\n",
    "X_val = df[df['date']>=test_start].copy()\n",
    "y_val = y[y['date']>=test_start].copy()\n",
    "dates_val = y_val['date']\n",
    "\n",
    "y_train = y_train['sales']\n",
    "y_val = y_val['sales']\n",
    "\n",
    "X_train = X_train[features]\n",
    "X_val = X_val[features]\n",
    "\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "cat_cols = ['unique_id']\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_val[col] = X_val[col].astype('category')\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=127,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=1000,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train_log,\n",
    "    eval_set=[(X_val, y_val_log)],\n",
    "    callbacks=[early_stopping(stopping_rounds=50), log_evaluation(period=100)]\n",
    ")\n",
    "y_pred_log = model.predict(X_val)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_pred = np.maximum(0, y_pred)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, y_pred)))\n",
    "rmsle\n",
    "#model = LGBMRegressor(\n",
    "#     objective='regression',\n",
    "#     metric='rmse',                   \n",
    "#     boosting_type='gbdt',            \n",
    "#     num_leaves=64,                   \n",
    "#     max_depth=7,                     \n",
    "#     learning_rate=0.05,              \n",
    "#     n_estimators=1000,               \n",
    "#     random_state=1\n",
    "# )\n",
    "# model.fit(\n",
    "#     X_train, y_train,\n",
    "#     eval_set=[(X_val, y_val)],\n",
    "#     callbacks=[early_stopping(stopping_rounds=50), log_evaluation(period=100)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_val, np.maximum(0, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({\n",
    "    'date': dates_val,\n",
    "    'actual': y_val,\n",
    "    'predicted': y_pred\n",
    "})\n",
    "\n",
    "daily_summary = df_plot.groupby('date')[['actual', 'predicted']].sum().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_summary['date'], daily_summary['actual'], label='Πραγματικές Πωλήσεις')\n",
    "plt.plot(daily_summary['date'], daily_summary['predicted'], label='Προβλεπόμενες Πωλήσεις')\n",
    "plt.xlabel('Ημερομηνία')\n",
    "plt.ylabel('Πωλήσεις')\n",
    "plt.title('Σύγκριση Ημερήσιων Πραγματικών και Προβλεπόμενων Πωλήσεων')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (split count)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': model.feature_name_,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=True)\n",
    "\n",
    "# Horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Split Count')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e6f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c05adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['unique_id'].isin(zero_sales['unique_id'])]\n",
    "df= df.drop(event_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['date','sales']]\n",
    "features = [\n",
    " 'onpromotion',\n",
    " 'day_nbr',\n",
    " 'year',\n",
    " 'month',\n",
    " 'holiday_national_binary',\n",
    " 'unique_id',\n",
    " 'oil_lag_1',\n",
    " 'lag_7',\n",
    " 'lag_14',\n",
    " 'lag_28',\n",
    " 'rolling_mean_7',\n",
    " 'rolling_mean_14',\n",
    " 'rolling_std_7',\n",
    " 'oil_lag_7',\n",
    " 'oil_lag_14',\n",
    " #'family',                                  # αυτα ειναι ολα τα features που τεσταρα αλλα τελικα δεν κρατησα\n",
    " #'is_weekend',\n",
    " #'store_nbr',\n",
    " #'city',\n",
    " #'state',\n",
    " #'type',\n",
    " #'cluster',\n",
    " #'quarter',\n",
    " #'holiday_local_binary',\n",
    " #'holiday_regional_binary',\n",
    " #'lag_1',\n",
    " #'trans_lag_1',                                    # Αυτα ηταν lags για τα transactions\n",
    " #'trans_lag_7',\n",
    " #'trans_lag_14'\n",
    "]\n",
    "X = df[features].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c638dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_per_uid[avg_sales_per_uid['avg_sales']<= 2.89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start='2017-07-15'\n",
    "X_train = df[df['date']<test_start].copy()\n",
    "y_train = y[y['date']<test_start].copy()\n",
    "\n",
    "X_val = df[df['date']>=test_start].copy()\n",
    "y_val = y[y['date']>=test_start].copy()\n",
    "dates_val = y_val['date']\n",
    "y_train = y_train['sales']\n",
    "y_val = y_val['sales']\n",
    "\n",
    "\n",
    "X_train = X_train[features]\n",
    "X_val = X_val[features]\n",
    "\n",
    "clip_threshold = y_train.quantile(0.995)\n",
    "y_train = y_train.clip(upper=clip_threshold)\n",
    "y_val= y_val.clip(upper=clip_threshold)\n",
    "\n",
    "cat_cols = ['unique_id']\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_val[col] = X_val[col].astype('category')\n",
    "\n",
    "reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree', \n",
    "                        enable_categorical=True,  \n",
    "                        n_estimators=1000,\n",
    "                        early_stopping_rounds=50,\n",
    "                        objective='reg:linear',\n",
    "                        max_depth=7,\n",
    "                        learning_rate=0.05,\n",
    "                        random_state=1\n",
    "                        )\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        verbose=100)\n",
    "\n",
    "y_pred = reg.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    'date': dates_val,\n",
    "    'actual': y_val,\n",
    "    'predicted': y_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de152e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_clipped = np.maximum(y_val, 0)\n",
    "y_pred_clipped = np.maximum(y_pred, 0)\n",
    "\n",
    "r2 = r2_score(y_val_clipped, y_pred_clipped)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_val_clipped, y_pred_clipped))\n",
    "rmse = np.sqrt(mean_squared_error(y_val_clipped, y_pred_clipped))\n",
    "print(r2,rmsle,rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(reg, max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee119d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({\n",
    "    'date': dates_val,\n",
    "    'actual': y_val_clipped,\n",
    "    'predicted': y_pred_clipped\n",
    "})\n",
    "\n",
    "daily_summary = df_plot.groupby('date')[['actual', 'predicted']].sum().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_summary['date'], daily_summary['actual'], label='Πραγματικές Πωλήσεις')\n",
    "plt.plot(daily_summary['date'], daily_summary['predicted'], label='Προβλεπόμενες Πωλήσεις')\n",
    "plt.xlabel('Ημερομηνία')\n",
    "plt.ylabel('Πωλήσεις')\n",
    "plt.title('Σύγκριση Ημερήσιων Πραγματικών και Προβλεπόμενων Πωλήσεων')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da50de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df.groupby(['date','unique_id']).agg({'sales':'sum'}).reset_index().sort_values(['unique_id','date'])\n",
    "total_sales_df = agg_df.pivot(index='date',columns='unique_id', values='sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed825c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_df.columns.tolist().index('45_BEVERAGES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39db9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_per_uid.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "i = 0\n",
    "for feature in total_sales_df.columns[1216:1217]:\n",
    "    selected_feature = str(feature)\n",
    "    category_df = total_sales_df[feature].copy().reset_index()\n",
    "    temp = df[df['unique_id'] == selected_feature].copy()\n",
    "    category_df.columns = [\"date\", \"sales\"]\n",
    "\n",
    "    temp = temp.drop(['id', 'store_nbr', 'family',\n",
    "        'sales','city','state', 'type', 'cluster',\n",
    "        'dcoilwtico', 'transactions','day','month_name',\n",
    "        'holiday_national','holiday_regional',\n",
    "        'holiday_local', 'event','unique_id'],axis=1)\n",
    "\n",
    "    category_df[\"date\"] = pd.to_datetime(category_df[\"date\"])\n",
    "    category_df= pd.merge(category_df,temp,on='date',how='left')\n",
    "    category_df=category_df.drop_duplicates()\n",
    "    category_df= category_df.drop_duplicates(subset=['date', 'sales'])\n",
    "    category_df=category_df.dropna()\n",
    "\n",
    "    y = category_df[['date', 'sales']]\n",
    "    features= category_df.columns.tolist()[2:]\n",
    "\n",
    "    test_start='2017-07-15'\n",
    "    X_train = category_df[category_df['date']<test_start].copy()\n",
    "    y_train = y[y['date']<test_start].copy()\n",
    "\n",
    "    X_val = category_df[category_df['date']>=test_start].copy()\n",
    "    y_val = y[y['date']>=test_start].copy()\n",
    "    dates_val = y_val['date']\n",
    "\n",
    "    y_train = y_train.drop(['date'],axis=1)\n",
    "    y_val = y_val.drop(['date'],axis=1)\n",
    "\n",
    "    X_train = X_train[features]\n",
    "    X_val = X_val[features]\n",
    "\n",
    "    reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree', \n",
    "                            enable_categorical=True,  \n",
    "                            n_estimators=1000,\n",
    "                            early_stopping_rounds=50,\n",
    "                            objective='reg:linear',\n",
    "                            max_depth=7,\n",
    "                            learning_rate=0.05,\n",
    "                            random_state=1\n",
    "                            )\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "            verbose=100)\n",
    "\n",
    "    y_pred = reg.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, y_pred)))\n",
    "\n",
    "    m=category_df['sales'].mean()\n",
    "    temp1 = pd.DataFrame({\n",
    "                \"unique_id\":[selected_feature],\n",
    "                \"rmse\":[rmse],\n",
    "                \"ymean\": [m],\n",
    "                \"rmsle\":[rmsle],\n",
    "                })\n",
    "    results.append(temp1)\n",
    "    print(f\"✅-{i}\")\n",
    "    i += 1\n",
    "results=pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_rmsle_count = (results[\"rmsle\"] > 0.5).sum()\n",
    "print(\"RMSLE > 0.5:\", high_rmsle_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fc3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({\n",
    "    'date': dates_val.unique(),\n",
    "    'actual': y_val['sales'].values,\n",
    "    'predicted': y_pred\n",
    "})\n",
    "\n",
    "daily_summary = df_plot.groupby('date')[['actual', 'predicted']].sum().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_summary['date'], daily_summary['actual'], label='Πραγματικές Πωλήσεις')\n",
    "plt.plot(daily_summary['date'], daily_summary['predicted'], label='Προβλεπόμενες Πωλήσεις')\n",
    "plt.xlabel('Ημερομηνία')\n",
    "plt.ylabel('Πωλήσεις')\n",
    "plt.title('Σύγκριση Ημερήσιων Πραγματικών και Προβλεπόμενων Πωλήσεων')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4307338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_score=0.5, booster='gbtree', \n",
    "#                         enable_categorical=True,  \n",
    "#                         n_estimators=1000,\n",
    "#                         early_stopping_rounds=50,\n",
    "#                         objective='reg:linear',\n",
    "#                         max_depth=7,\n",
    "#                         learning_rate=0.05,\n",
    "#Αυτα ειχαν 201 RMSE\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [7],\n",
    "    'learning_rate': [0.05,0.1],\n",
    "    'n_estimators': [1000],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1aabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in ParameterGrid(param_grid):\n",
    "    model = xgb.XGBRegressor(\n",
    "        base_score=0.5,\n",
    "        booster='gbtree',\n",
    "        enable_categorical=True,\n",
    "        objective='reg:linear',\n",
    "        early_stopping_rounds=50,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val)\n",
    "    score = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    print(f\"Params: {params} --> RMSE: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236498da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['date']=='2017-07-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NHITS\n",
    "\n",
    "# Θελω για αρχη να τρεξω το μοντελο μονο με τις static μεταβλητες\n",
    "df = df.drop([ 'id','onpromotion', 'city','state', 'type', 'cluster', 'dcoilwtico', 'transactions', 'day','holiday_national', 'holiday_regional', 'holiday_local', 'event',\n",
    "       'holiday_national_binary', 'holiday_regional_binary',\n",
    "       'holiday_local_binary',],axis=1)\n",
    "df['unique_id'] = df['store_nbr'].astype(str) + '_' + df['family']\n",
    "df['ds'] = pd.to_datetime(df['date'])\n",
    "df['y'] = df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36419341",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4736490",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.groupby(['unique_id', 'ds']).size().reset_index(name='count')\n",
    "duplicates = duplicates[duplicates['count'] > 1]\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"✅ Κάθε unique_id εμφανίζεται το πολύ μία φορά ανά ημερομηνία.\")\n",
    "else:\n",
    "    print(\"❌ Υπάρχουν διπλότυπα:\")\n",
    "    display(duplicates.head())\n",
    "\n",
    "df_agg = df.groupby(['unique_id', 'ds'], as_index=False)['y'].sum()\n",
    "df = df_agg\n",
    "\n",
    "# Split into train and future (NHITS uses sliding windows, not test sets in the usual sense)\n",
    "df_train = df.groupby('unique_id').apply(lambda x: x.iloc[:-30]).reset_index(drop=True)\n",
    "df_test = df.groupby('unique_id').apply(lambda x: x.iloc[-30:]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "model = NeuralForecast(\n",
    "    models=[NHITS(input_size=90, h=30, max_steps=2000, scaler_type='standard', loss=RMSE(),learning_rate=5e-5)],\n",
    "    freq='D',\n",
    ")\n",
    "\n",
    "# model = NeuralForecast(models=[NHITS(input_size=2*H, h=H)], freq='D')\n",
    "# Fit the model\n",
    "model.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "df_forecast = model.predict()\n",
    "\n",
    "# Rename 'y' to 'y_actual' in df_test to match expected column name after merge\n",
    "df_test_renamed = df_test.rename(columns={'y': 'y_actual'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a01f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge forecasts and actuals\n",
    "df_eval = pd.merge(\n",
    "    df_forecast, \n",
    "    df_test_renamed, \n",
    "    on=['unique_id', 'ds'], \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,mean_squared_log_error\n",
    "import numpy as np\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(df_eval['y_actual'], df_eval['NHITS'])\n",
    "rmse = np.sqrt(mean_squared_error(df_eval['y_actual'], df_eval['NHITS']))\n",
    "#rmsle = np.sqrt(mean_squared_log_error(df_eval['y_actual'], df_eval['NHITS']))\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"RMSE:  {rmse:.2f}\")\n",
    "#print(f\"RMSLE:  {rmsle:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast= df_forecast.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(df,df_forecast, max_insample_length=24*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(df[df['unique_id'].isin(['10_AUTOMOTIVE', '2_BEVERAGES'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure both components are strings\n",
    "# df['store_nbr'] = df['store_nbr'].astype(str)\n",
    "# df['family'] = df['family'].astype(str)\n",
    "\n",
    "# # Create the unique_id column\n",
    "# df['unique_id'] = df['store_nbr'] + '_' + df['family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.rename(columns={'date': 'ds'})\n",
    "# df_forecast = df[['ds', 'sales', 'unique_id']].rename(columns={'sales': 'y'})\n",
    "# df_forecast = df_forecast.set_index('unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a12786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2017 = df_forecast[df_forecast['ds'].dt.year == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d752839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2017 = df_2017.reset_index()\n",
    "# df_2017.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2017.to_csv(r\"/mnt/c/Users/aimeraj/Desktop/data2/df_2017.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_df = static_df.set_index('unique_id')\n",
    "# static_df =static_df.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_nhits = {\n",
    "#     \"input_size\": tune.choice([48, 96, 144]),            \n",
    "#     \"start_padding_enabled\": True,\n",
    "#     \"n_blocks\": [1, 1, 1, 1, 1],                         \n",
    "#     \"mlp_units\": [[64, 64]] * 5,                         \n",
    "#     \"n_pool_kernel_size\": tune.choice([\n",
    "#         [1, 1, 1, 1, 1],\n",
    "#         [2, 2, 2, 2, 2],\n",
    "#         [4, 4, 4, 4, 4],\n",
    "#         [8, 4, 2, 1, 1]\n",
    "#     ]),\n",
    "#     \"n_freq_downsample\": tune.choice([\n",
    "#         [8, 4, 2, 1, 1],\n",
    "#         [1, 1, 1, 1, 1]\n",
    "#     ]),\n",
    "#     \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "#     \"scaler_type\": None,\n",
    "#     \"max_steps\": 1000,\n",
    "#     \"batch_size\": tune.choice([1, 4, 10]),\n",
    "#     \"windows_batch_size\": tune.choice([128]),\n",
    "#     \"random_seed\": tune.randint(1, 21),                   \n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizon = 15\n",
    "# model = [\n",
    "#     NHITS(\n",
    "#         h=horizon,\n",
    "#         input_size= 10 * horizon,        # Use 75 past days (tune this!)\n",
    "#         max_steps=100,                   # Total training steps (tune based on dataset size)\n",
    "#         stat_exog_list=static_df.columns.tolist(),  # List of static variable names\n",
    "#         scaler_type='robust',            # Robust scaling handles outliers better\n",
    "#         loss=MQLoss(),                   # Quantile loss (good for uncertainty)\n",
    "#         n_blocks=[1, 1, 1],              # Simplify if training is slow\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9acb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Διαφορα πραγματα που ειχα γραψει προσπαθωντας να τρεξω NHITS \n",
    "### Τα εκανα comment out επειδη δεν ειναι με σειρα ωστε να δουλευουν\n",
    "\n",
    "# df_2017.head()\n",
    "# df_2017 = df_2017.loc[\"1_AUTOMOTIVE\"]\n",
    "# static_df = static_df[['1_AUTOMOTIVE']]\n",
    "# nf = NeuralForecast(models=model, freq='D')\n",
    "# nf.fit(df=df_2017, static_df=static_df)\n",
    "# Y_hat_df = nf.predict()\n",
    "# # Όταν κάνεις reset_index, βάλε το σωστό όνομα:\n",
    "# df_2017_reset = df_2017.reset_index().rename(columns={'index': 'unique_id'})\n",
    "# df_2017 = df_2017.reset_index()\n",
    "# # Βήμα 1: Reset το df σου\n",
    "# df_2017_reset = df_2017.reset_index()\n",
    "\n",
    "# # Βήμα 2: Reset και το forecast\n",
    "# Y_hat_df = Y_hat_df.reset_index()\n",
    "\n",
    "# # Βήμα 3: Τσέκαρε τι στήλες έχουν\n",
    "# print(df_2017_reset.columns)\n",
    "# print(Y_hat_df.columns)\n",
    "\n",
    "# # Βήμα 4: Plot\n",
    "# plot_series(df_2017_reset, Y_hat_df, max_insample_length=24*5)\n",
    "\n",
    "# df_2017_reset = df_2017.reset_index()  # φέρνει το unique_id ως στήλη\n",
    "# assert set(df_2017.index.unique()) <= set(static_df.index), \"Mismatch in unique_ids!\"\n",
    "\n",
    "# missing_ids = set(df_2017.index.unique()) - set(static_df.index)\n",
    "# print(missing_ids)\n",
    "\n",
    "# for missing_id in missing_ids:\n",
    "#     static_df.loc[missing_id] = [0] * static_df.shape[1]  # ή βάλε default τιμές\n",
    "\n",
    "# nf = NeuralForecast(models=model, freq='D')  # Daily frequency\n",
    "# nf.fit(df= df_2017, static_df=static_df)\n",
    "\n",
    "# df_forecast.head()\n",
    "# df_2017 = df_forecast[df_forecast['ds'].dt.year == 2017]\n",
    "# del df_forecast\n",
    "\n",
    "# nf = NeuralForecast(models=model, freq='D')\n",
    "#nf.fit(df=df_2017, static_df=static_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
